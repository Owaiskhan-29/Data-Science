{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    ">>> A decision tree is a tree-like structure that is used as a model for classifying data.    \n",
    "\n",
    "    As a marketing manager, you want a set of customers who are most likely to purchase your product. This is how you can save your marketing budget by finding your audience. As a loan manager, you need to identify risky loan applications to achieve a lower loan default rate. This process of classifying customers into a group of potential and non-potential customers or safe or risky loan applications is known as a classification problem. Classification is a two-step process, learning step and prediction step. In the learning step, the model is developed based on given training data. In the prediction step, the model is used to predict the response for given data. Decision Tree is one of the easiest and popular classification algorithms to understand and interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       Regression-type problems.\n",
    "       \n",
    "           Regression-type problems are generally those where we attempt to predict the values of a continuous variable from one or more continuous and/or categorical predictor variables. For example, we may want to predict the selling prices of single-family homes (a continuous dependent variable) from various other continuous predictors (e.g., square footage) as well as categorical predictors (e.g., style of home, such as ranch, two-story, etc.; zip code or telephone area code where the property is located, etc.; note that this latter variable would be categorical in nature, even though it would contain numeric values or codes). If we used simple multiple regression, or some general linear model (GLM) to predict the selling prices of single-family homes, we would determine a linear equation for these variables that can be used to compute predicted selling prices.\n",
    "           \n",
    "       Classification-type problems. \n",
    "       \n",
    "           Classification-type problems are generally those where we attempt to predict values of a categorical dependent variable (class, group membership, etc.) from one or more continuous and/or categorical predictor variables. For example, we may be interested in predicting who will or will not graduate from college, or who will or will not renew a subscription. These would be examples of simple binary classification problems, where the categorical dependent variable can only assume two distinct and mutually exclusive values. In other cases, we might be interested in predicting which one of multiple different alternative consumer products (e.g., makes of cars) a person decides to purchase, or which type of failure occurs with different types of engines. In those cases, there are multiple categories or classes for the categorical dependent variable.\n",
    "           \n",
    "![](classy.png)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Decision Tree Algorithm\n",
    "    A decision tree is a flowchart-like tree structure where an internal node represents feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. It partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision making. It’s visualization like a flowchart diagram which easily mimics the human-level thinking. That is why decision trees are easy to understand and interpret.\n",
    "    \n",
    "    Decision trees classify the examples by sorting them down the tree from the root to some leaf node, with the leaf node providing the classification to the example, this approach is called a Top-Down approach. Each node in the tree acts as a test case for some attribute, and each edge descending from that node corresponds to one of the possible answers to the test case. This process is recursive and is repeated for every subtree rooted at the new nodes.\n",
    "    \n",
    "![](tree.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we dive deep, let’s get familiar with some of the terminologies:\n",
    "    1. Root Node (Top Decision Node): It represents the entire population or sample and this further gets divided into two or more homogeneous sets.\n",
    "    2. Splitting: It is a process of dividing a node into two or more sub-nodes.\n",
    "    3. Decision Node: When a sub-node splits into further sub-nodes, then it is called a decision node.\n",
    "    4. Leaf/ Terminal Node: Nodes with no children (no further split) is called Leaf or Terminal node.\n",
    "    5. Pruning: When we reduce the size of decision trees by removing nodes (opposite of Splitting), the process is called pruning.\n",
    "    6. Branch / Sub-Tree: A sub section of the decision tree is called branch or sub-tree.\n",
    "    7. Parent and Child Node: A node, which is divided into sub-nodes is called a parent node of sub-nodes whereas sub-nodes are the child of a parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the Decision Tree algorithm work?\n",
    "\n",
    "    The basic idea behind any decision tree algorithm is as follows:\n",
    "    \n",
    "        1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n",
    "        2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n",
    "        3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n",
    "        \n",
    "                o All the tuples belong to the same attribute value.\n",
    "                o There are no more remaining attributes.\n",
    "                o There are no more instances.\n",
    "                \n",
    "![](treeprocess.jpg)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute Selection Measures (ASM)\n",
    "    Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute. In the case of a continuous-valued attribute, split points for branches also need to define. Most popular selection measures are Information Gain, Gain Ratio, and Gini Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain:\n",
    "    Information gain is a statistical property that measures how well a given attribute separates the training examples (data) according to their target classification. Look at the image below and think which node can be described easily. I am sure, your answer is C because it requires less information as all values are similar. On the other hand, B requires more information to describe it and A requires the maximum information. In other words, we can say that C is a Pure node, B is less Impure and A is more impure.\n",
    "    \n",
    "![](infogain.png)   \n",
    "\n",
    "    Now, we can build a conclusion that less impure node requires less information to describe it. And, the more impure node requires more information.\n",
    "    \n",
    "    In a parallel example considering Information Gain, In the figure below, we can see that an attribute with low information gain (bottom) splits the data relatively evenly and as a result doesn’t bring us any closer to a decision. Whereas, an attribute with high information gain (top) splits the data into groups with an uneven number of positives and negatives and as a result, helps in separating the two from each other.\n",
    "    \n",
    "![](infogain2.png)    \n",
    "\n",
    "    To be able to calculate the information gain, we have to first introduce the term entropy of a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "    which measures the impurity of the input set. In physics and mathematics, entropy referred to as the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information gain is a decrease in entropy.\n",
    "    \n",
    "    The idea behind the entropy is, in simplified terms, the following: Imagine you have a lottery wheel which includes 100 green balls. The set of balls within the lottery wheel can be said to be totally pure because only green balls are included. To express this in the terminology of entropy, this set of balls has an entropy of 0 (we can also say zero impurity). Consider now, 30 of these balls are replaced by red and 20 by blue balls.\n",
    "    \n",
    "![](purity.png)  \n",
    "\n",
    "    If you now draw another ball from the lottery wheel, the probability of receiving a green ball has dropped from 1.0 to 0.5. Since the impurity increased, the purity decreased, hence also the entropy increased. Hence we can say, the more “impure” a dataset, the higher the entropy and the less “impure” a dataset, the lower the entropy.\n",
    "    \n",
    "    Note that entropy is 0 if all the members of S belong to the same class. For example, if all members are positive, Entropy(S) = 0. Entropy is 1 when the sample contains an equal number of positive and negative examples. If the sample contains an unequal number of positive and negative examples, entropy is between 0 and 1. The following figure shows the form of the entropy function relative to a boolean classification as entropy varies between 0 and 1.\n",
    "    \n",
    "![](entropy.jpg)\n",
    "\n",
    "    Entropy can be calculated using the formula:-\n",
    "\n",
    "![](entropyf.png)\n",
    "\n",
    "    Here p and q is the probability of success and failure respectively in that node. Entropy is also used with the categorical target variable. It chooses the split which has the lowest entropy compared to the parent node and other splits. The lesser the entropy, the better it is.\n",
    "    \n",
    "    Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values\n",
    "    \n",
    ">>> Information Gain = Entropy(parent node) — Avg Entropy(children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to calculate entropy for a split:\n",
    "    1. Calculate the entropy of the parent node\n",
    "    2. Calculate entropy of each individual node of split and calculate the weighted average of all sub-nodes available in a split.\n",
    "    \n",
    "    Example: Let’s say we have a sample of 30 students with three variables Gender (Boy/ Girl), Class( IX/ X) and Height (5 to 6 ft). 15 out of these 30 play cricket in leisure time. Now, I want to create a model to predict who will play cricket during a leisure period? In this problem, we need to segregate students who play cricket in their leisure time based on highly significant input variable among all three.\n",
    "    \n",
    "![](entropyexample.png)    \n",
    "\n",
    "    Entropy for parent node = -(15/30) log2 (15/30) — (15/30) log2 (15/30) = 1\n",
    "    \n",
    "    Here 1 shows that it is a impure node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Split on gender:\n",
    "    Entropy for Female node = -(2/10) log2 (2/10) — (8/10) log2 (8/10) = 0.72 \n",
    "    Entropy for Male node = -(13/20) log2 (13/20) — (7/20) log2 (7/20) = 0.93 \n",
    "    \n",
    "    Entropy for split Gender = (10/30)*0.72 + (20/30)*0.93 = 0.86\n",
    "    \n",
    "    Information Gain for split on gender = 1–0.86 = 0.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Split on Class:\n",
    "\n",
    "    Entropy for Class IX node = -(6/14) log2 (6/14) — (8/14) log2 (8/14) = 0.99 \n",
    "    Entropy for Class X node = -(9/16) log2 (9/16) — (7/16) log2 (7/16) = 0.99 \n",
    "    \n",
    "    Entropy for split Class = (14/30)*0.99 + (16/30)*0.99 = 0.99\n",
    "    \n",
    "    Information Gain for split on Class = 1– 0.99 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Above, you can see that Information Gain for Split on Gender is the Highest among all, so the tree will split on Gender."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini\n",
    "\n",
    "    Gini says, if we select two items from a population at random then they must be of same class and probability for this is 1 if population is pure.\n",
    "    \n",
    "    > It works with categorical target variable “Success” or “Failure”.\n",
    "    > It performs only Binary splits\n",
    "    > Higher the value of Gini higher the homogeneity.\n",
    "    > CART (Classification and Regression Tree) uses the Gini method to create binary splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to Calculate Gini for a split\n",
    "    \n",
    "    Calculate Gini for sub-nodes, using formula sum of the square of probability for success and failure (p²+q²).\n",
    "    \n",
    "    Calculate Gini for split using weighted Gini score of each node of that split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Example: — Referring to the example used above, where we want to segregate the students based on the target variable ( playing cricket or not ). In the snapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini.\n",
    "    \n",
    "![](entropyexample.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split on Gender:\n",
    "\n",
    "    Calculate, Gini for sub-node Female = (0.2)*(0.2)+(0.8)*(0.8)=0.68\n",
    "   \n",
    "    Gini for sub-node Male = (0.65)*(0.65)+(0.35)*(0.35)=0.55\n",
    "    \n",
    "    Calculate weighted Gini for Split Gender = (10/30)*0.68+(20/30)*0.55 = 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similar for Split on Class:\n",
    "   \n",
    "    Gini for sub-node Class IX = (0.43)*(0.43)+(0.57)*(0.57)=0.51\n",
    "    \n",
    "    Gini for sub-node Class X = (0.56)*(0.56)+(0.44)*(0.44)=0.51\n",
    "    \n",
    "    Calculate weighted Gini for Split Class = (14/30)*0.51+(16/30)*0.51 = 0.51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Above, you can see that Gini score for Split on Gender is higher than Split on Class, hence, the node split will take place on Gender.\n",
    "\n",
    "    You might often come across the term ‘Gini Impurity’ which is determined by subtracting the Gini value from 1. So mathematically we can say,\n",
    "    \n",
    "    Gini Impurity = 1-Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>> Gini is default parameter in Decision Tree Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
