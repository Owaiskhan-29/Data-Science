{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
    "    \n",
    "    Clustering algorithms use different distance or similarity or dissimilarity measures to derive different clusters. The type of distance/similarity measure used plays a crucial role in the final cluster formation. Larger distance would imply that observations are far away from one another. Whereas higher similarity would indicate that the observations are similar. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is one of the frequently used clustering algorithms. It is a non-hierarchical clustering method in which the number of clusters(K) is decided a priori. The observations in the sample are assigned to one of the clusters (say C1, C2, ... Ck) based on the \"distance\" between the observation and the centroid of the clusters.\n",
    "\n",
    "The following steps are used in K-means clustering algorithm:\n",
    "\n",
    "1. Decide the value of K (which can be fin-tuned later).\n",
    "2. Choose K observations from the data that are likely to be in different clusters. There are many ways of choosing these initial K values; the easiest approach is to choose observations that are farthest. \n",
    "3. The K observations selected in step 2 are the centroids of those clusters.\n",
    "4. For remaining observations, find the cluster closest to the centroid. Add the new observation to the cluster with the closest centroid. Adjust the centroid after adding a new observation to the cluster. The closest centroid is chosen based upon an appropriate distance measure.\n",
    "5. Repeat step 4 until all observations are assigned to a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    It can be observed that there are three customer segments.\n",
    "        \n",
    "        > One on the top-left of the graph, depicting low-age-high-income group\n",
    "        > One on the top-right side of the graph, depicting high-age-medium-income group\n",
    "        > One on the bottom of the graph, depicting a low-income group, which has an age spread from low to high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does KMeans clustering works?\n",
    "\n",
    "     Imagine we have these gray points in the following figure and want to assign them into three clusters. K-means follows the four steps listed below.\n",
    "     \n",
    "![](cluster1.png)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step one: Initialize cluster centers\n",
    "    We randomly pick three points C1, C2 and C3, and label them with blue, green and red color separately to represent the cluster centers.\n",
    "    \n",
    "![](cluster2.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step two: Assign observations to the closest cluster center\n",
    "    \n",
    "![](cluster3.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Once we have these cluster centers, we can assign each point to the clusters based on the minimum distance to the cluster center. For the gray point A, compute its distance to C1, C2 and C3, respectively. And after comparing the lengths of d1, d2 and d3, we figure out that d1 is the smallest, therefore, we assign point A to the blue cluster and label it with blue. We then move to point B and follow the same procedure. This process can assign all the points and leads to the following figure.\n",
    "\n",
    "\n",
    "![](cluster4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step three: Revise cluster centers as mean of assigned observations\n",
    "\n",
    "    Now we’ve assigned all the points based on which cluster center they were closest to. Next, we need to update the cluster centers based on the points assigned to them. For instance, we can find the center mass of the blue cluster by summing over all the blue points and dividing by the total number of points, which is four here. And the resulted center mass C1’, represented by a blue diamond, is our new center for the blue cluster. Similarly, we can find the new centers C2’ and C3’ for the green and red clusters.\n",
    "    \n",
    "![](cluster5.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Step four: Repeat step 2 and step 3 until convergence\n",
    "    \n",
    "    The last step of k-means is just to repeat the above two steps. For example, in this case, once C1’, C2’ and C3’ are assigned as the new cluster centers, point D becomes closer to C3’ and thus can be assigned to the red cluster. We keep on iterating between assigning points to cluster centers, and updating the cluster centers until convergence. Finally, we may get a solution like the following figure. Well done!\n",
    "    \n",
    "![](cluster6.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "    A dendrogram is a diagram that shows the hierarchical relationship between objects. It is most commonly created as an output from hierarchical clustering. The main use of a dendrogram is to work out the best way to allocate objects to clusters. The dendrogram below shows the hierarchical clustering of six observations shown on the scatterplot to the left.\n",
    "    \n",
    "![](dendrogram.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In the dendrogram above, the height of the dendrogram indicates the order in which the clusters were joined. A more informative dendrogram can be created where the heights reflect the distance between the clusters as is shown below. In this case, the dendrogram shows us that the big difference between clusters is between the cluster of A and B versus that of C, D, E, and F."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
